{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8968421,"sourceType":"datasetVersion","datasetId":4066836},{"sourceId":11542250,"sourceType":"datasetVersion","datasetId":7238381}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Ensure Ultralytics is installed in the Kaggle environment\n!pip install ultralytics pyyaml tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:46:31.434618Z","iopub.execute_input":"2025-04-24T08:46:31.434781Z","iopub.status.idle":"2025-04-24T08:48:00.244805Z","shell.execute_reply.started":"2025-04-24T08:46:31.434765Z","shell.execute_reply":"2025-04-24T08:48:00.244031Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.115-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.115-py3-none-any.whl (983 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.5/983.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.115 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Snippet 1: Setup, Installs, Config, Basic Helpers ---\n\n!pip install ultralytics pyyaml tqdm matplotlib seaborn tidecv # Use latest available tidecv\nimport os\nimport cv2\nimport numpy as np\nimport yaml\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ultralytics import YOLO\nfrom ultralytics.utils.metrics import box_iou\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport shutil\nimport logging\nimport json\nfrom tidecv import TIDE, datasets # Import TIDE library\n\n# --- Configuration ---\nMODEL_PATH = Path(\"/kaggle/input/best-model-30/best.pt\")\nDATASET_YAML_PATH = Path(\"/kaggle/input/cardetection/car/data.yaml\")\nOUTPUT_DIR = Path(\"/kaggle/working/robust_error_analysis\") # New output dir name\nSPLIT_TO_ANALYZE = 'valid' # Or 'test'\nCONF_THRESHOLD = 0.25 # Standard validation confidence\nIOU_THRESHOLD = 0.50 # Standard IoU for matching\n\n# --- Analysis Specific Config ---\n# Set to a class name like \"Green Light\" to analyze its FPs, or None to disable\nTARGET_FP_ANALYSIS_CLASS_NAME = \"Green Light\"\n# Brightness thresholds for slicing (0-255 scale)\nBRIGHTNESS_DARK_THRESHOLD = 70\nBRIGHTNESS_BRIGHT_THRESHOLD = 180\n# Confidence bins for analysis\nCONFIDENCE_BINS = [0.0, 0.3, 0.5, 0.7, 0.9, 1.0]\n\n# --- Setup Logging ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s-%(levelname)s: %(message)s')\nlogging.getLogger('matplotlib').setLevel(logging.WARNING) # Quieter plots\n\n# --- Basic Helper Functions ---\n\ndef setup_output_dirs(base_dir):\n    \"\"\"Creates necessary output directories, removing old ones.\"\"\"\n    logging.info(f\"Setting up output directories under {base_dir}\")\n    if base_dir.exists():\n        logging.warning(f\"Removing previous analysis directory: {base_dir}\")\n        shutil.rmtree(base_dir)\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    dirs = {\n        \"errors_fp\": base_dir / \"error_images\" / \"false_positives\",\n        \"errors_fn\": base_dir / \"error_images\" / \"false_negatives\",\n        \"errors_misc\": base_dir / \"error_images\" / \"misclassified\",\n        \"fp_color_analysis\": base_dir / \"fp_color_analysis\",\n        \"tide_results\": base_dir / \"tide_results\",\n        \"slice_analysis\": base_dir / \"slice_analysis\",\n        \"confidence_analysis\": base_dir / \"confidence_analysis\"\n    }\n    for d in dirs.values():\n        d.mkdir(parents=True, exist_ok=True)\n    logging.info(\"Output directories created.\")\n    return dirs\n\ndef load_dataset_info(yaml_path, split):\n    \"\"\"Loads dataset paths and class names from YAML file (more flexible path handling).\"\"\"\n    logging.info(f\"Attempting to load dataset info from: {yaml_path}\")\n    if not yaml_path.exists():\n        logging.error(f\"Dataset YAML file not found at {yaml_path}\")\n        raise FileNotFoundError(f\"Dataset YAML file not found at {yaml_path}\")\n\n    logging.debug(f\"Reading YAML content...\")\n    with open(yaml_path, 'r') as f:\n        try:\n            data = yaml.safe_load(f)\n            logging.debug(f\"YAML content loaded: {data}\")\n        except yaml.YAMLError as e:\n            logging.error(f\"Error parsing YAML file: {e}\")\n            raise\n\n    if not isinstance(data, dict):\n        logging.error(f\"YAML content is not a dictionary. Content: {data}\")\n        raise ValueError(\"YAML content is not a dictionary.\")\n\n    # --- Class Names (Still required from YAML) ---\n    if 'names' not in data:\n        logging.error(f\"'names' key (class names) not found in YAML data.\")\n        raise ValueError(f\"'names' key (class names) not found in {yaml_path}\")\n    class_names = data['names']\n    logging.debug(f\"Class names found: {class_names}\")\n\n    # Determine the dataset root directory (where the yaml file is located)\n    dataset_root = yaml_path.parent\n    logging.debug(f\"Dataset root directory (location of YAML): {dataset_root}\")\n\n    # --- Flexible Path Finding ---\n    img_path = None\n    label_path = None\n    image_path_found = False\n    label_path_found = False\n\n    # Method 1: Check if 'split' is a key in YAML (standard Ultralytics way)\n    if split in data:\n        logging.info(f\"Split '{split}' found as a key in YAML. Using defined path.\")\n        img_path_relative = data[split]\n        logging.debug(f\"Relative image path for split '{split}' from YAML: {img_path_relative}\")\n        img_path_attempt = (dataset_root / img_path_relative).resolve()\n        logging.info(f\"Attempting path from YAML key: {img_path_attempt}\")\n        if img_path_attempt.exists() and img_path_attempt.is_dir():\n            img_path = img_path_attempt\n            image_path_found = True\n            logging.info(f\"Image path found via YAML key: {img_path}\")\n            # Try finding corresponding label path by replacing 'images'\n            label_path_attempt = Path(str(img_path).replace(\"images\", \"labels\", 1))\n            if label_path_attempt.exists() and label_path_attempt.is_dir():\n                 label_path = label_path_attempt\n                 label_path_found = True\n                 logging.info(f\"Corresponding label path found: {label_path}\")\n            else:\n                 logging.warning(f\"Found image path via YAML, but corresponding label path {label_path_attempt} not found.\")\n        else:\n             logging.warning(f\"Path '{img_path_attempt}' defined for key '{split}' in YAML not found or not a directory.\")\n\n    # Method 2: If YAML key didn't work OR wasn't present, assume standard structure\n    if not image_path_found:\n        logging.info(f\"Split '{split}' not found as key in YAML or path invalid. Assuming standard directory structure: {dataset_root}/{split}/images\")\n        img_path_attempt = (dataset_root / split / \"images\").resolve()\n        logging.info(f\"Attempting standard path: {img_path_attempt}\")\n        if img_path_attempt.exists() and img_path_attempt.is_dir():\n            img_path = img_path_attempt\n            image_path_found = True\n            logging.info(f\"Image path found via standard structure: {img_path}\")\n            # Check for corresponding labels dir\n            label_path_attempt = (dataset_root / split / \"labels\").resolve()\n            if label_path_attempt.exists() and label_path_attempt.is_dir():\n                label_path = label_path_attempt\n                label_path_found = True\n                logging.info(f\"Corresponding label path found: {label_path}\")\n            else:\n                logging.warning(f\"Found image path via standard structure, but corresponding label path {label_path_attempt} not found.\")\n        else:\n            logging.error(f\"Standard image path {img_path_attempt} also not found or not a directory.\")\n\n    # Final Check\n    if not image_path_found:\n        raise FileNotFoundError(f\"Could not locate image directory for split '{split}' using YAML key or standard structure relative to {dataset_root}.\")\n    if not label_path_found:\n        # Allow proceeding without labels for prediction-only tasks, but log error for analysis\n        logging.error(f\"Could not locate label directory for split '{split}' corresponding to {img_path}. Ground truth cannot be loaded.\")\n        raise FileNotFoundError(f\"Could not locate label directory for split '{split}' corresponding to {img_path}.\")\n\n\n    # Find image files in the located directory\n    allowed_extensions = ['.jpg', '.png', '.jpeg', '.bmp', '.tif', '.tiff', '.webp']\n    logging.debug(f\"Scanning {img_path} for images...\")\n    image_files = sorted([p for p in img_path.iterdir() if p.suffix.lower() in allowed_extensions])\n    logging.info(f\"Found {len(image_files)} images in {img_path}\")\n\n    if not image_files:\n         logging.warning(f\"No image files found in the directory: {img_path}. Check extensions and directory contents.\")\n\n    logging.info(f\"Using Image Path: {img_path}\")\n    logging.info(f\"Using Label Path: {label_path}\")\n    logging.info(f\"Class names ({len(class_names)}): {class_names}\")\n    logging.info(f\"Load_dataset_info finished successfully.\")\n    return image_files, label_path, class_names\n    \nprint(\"Snippet 1 (Setup & Basic Helpers) finished.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:21:42.379886Z","iopub.execute_input":"2025-04-24T09:21:42.380434Z","iopub.status.idle":"2025-04-24T09:21:45.650603Z","shell.execute_reply.started":"2025-04-24T09:21:42.380410Z","shell.execute_reply":"2025-04-24T09:21:45.649677Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.115)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: tidecv in /usr/local/lib/python3.11/dist-packages (1.0.1)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from tidecv) (1.4.4)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from tidecv) (2.0.8)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nSnippet 1 (Setup & Basic Helpers) finished.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# --- Snippet 2: Core Analysis & Visualization Helpers ---\nimport colorsys # For HSV conversion\n\n# --- Ground Truth & Coordinate Helpers --- (Refined from previous attempts)\ndef load_ground_truth(label_path, class_count):\n    \"\"\"Loads ground truth boxes [class_id, xc, yc, w, h] from a label file.\"\"\"\n    gt_boxes = []\n    if label_path.exists():\n        with open(label_path, 'r') as f:\n            for line_num, line in enumerate(f):\n                parts = line.strip().split()\n                if len(parts) == 5:\n                    try:\n                        class_id = int(parts[0])\n                        x_c, y_c, w, h = map(float, parts[1:])\n                        if not (0 <= class_id < class_count):\n                            logging.warning(f\"L{line_num+1}: Invalid class ID {class_id} in {label_path.name}. Max is {class_count-1}. Skipping.\")\n                            continue\n                        if not (0 <= x_c <= 1 and 0 <= y_c <= 1 and 0 < w <= 1 and 0 < h <= 1):\n                            logging.warning(f\"L{line_num+1}: Invalid coords [{x_c},{y_c},{w},{h}] in {label_path.name}. Skipping.\")\n                            continue\n                        gt_boxes.append([class_id, x_c, y_c, w, h])\n                    except ValueError:\n                        logging.warning(f\"L{line_num+1}: Cannot parse coords in {label_path.name}. Line: '{line.strip()}'. Skipping.\")\n                elif len(parts) > 0:\n                    logging.warning(f\"L{line_num+1}: Incorrect format ({len(parts)} parts) in {label_path.name}. Line: '{line.strip()}'. Skipping.\")\n    return np.array(gt_boxes) # Shape: (N, 5) -> [class_id, xc, yc, w, h]\n\n\ndef yolo_to_xyxy(boxes_yolo, img_width, img_height):\n    \"\"\"Converts YOLO [xc, yc, w, h] (normalized) to [x1, y1, x2, y2] (absolute).\"\"\"\n    boxes_xyxy = []\n    if boxes_yolo is None or len(boxes_yolo) == 0: return np.array(boxes_xyxy)\n    for box in boxes_yolo:\n        xc, yc, w, h = box # Assumes format [xc, yc, w, h, ...]\n        x1 = (xc - w / 2) * img_width\n        y1 = (yc - h / 2) * img_height\n        x2 = (xc + w / 2) * img_width\n        y2 = (yc + h / 2) * img_height\n        # Clamp to image bounds\n        x1, y1 = max(0.0, x1), max(0.0, y1)\n        x2, y2 = min(float(img_width - 1), x2), min(float(img_height - 1), y2)\n        if x1 < x2 and y1 < y2: # Ensure valid box after clamping\n             boxes_xyxy.append([x1, y1, x2, y2])\n        # else: logging.warning(f\"Invalid box after clamping: {box} -> {[x1,y1,x2,y2]}\")\n    return np.array(boxes_xyxy) # Shape: (N, 4) -> [x1, y1, x2, y2]\n\n\n# --- Image Condition Analysis Helpers ---\ndef calculate_brightness(image):\n    \"\"\"Calculates average brightness (grayscale value).\"\"\"\n    try:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        return np.mean(gray)\n    except cv2.error as e:\n        logging.error(f\"OpenCV error calculating brightness: {e}\")\n        return -1 # Indicate error\n\n\ndef calculate_contrast(image):\n    \"\"\"Calculates RMS contrast.\"\"\"\n    try:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        return gray.std()\n    except cv2.error as e:\n        logging.error(f\"OpenCV error calculating contrast: {e}\")\n        return -1 # Indicate error\n\n# --- Color Analysis Helpers ---\ndef get_box_roi(image, box_xyxy):\n    \"\"\"Safely extracts the Region of Interest for a bounding box.\"\"\"\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = map(int, box_xyxy[:4])\n    x1, y1 = max(0, x1), max(0, y1)\n    x2, y2 = min(w - 1, x2), min(h - 1, y2)\n    if x1 >= x2 or y1 >= y2: return None # Invalid box\n    return image[y1:y2, x1:x2]\n\ndef analyze_box_hsv(roi):\n    \"\"\"Calculates average Hue, Saturation, Value for a ROI.\"\"\"\n    if roi is None or roi.size == 0: return None\n    try:\n        hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n        # H ranges 0-179, S 0-255, V 0-255 in OpenCV\n        avg_h = np.mean(hsv_roi[:, :, 0])\n        avg_s = np.mean(hsv_roi[:, :, 1])\n        avg_v = np.mean(hsv_roi[:, :, 2])\n        return {'H': avg_h, 'S': avg_s, 'V': avg_v}\n    except cv2.error as e:\n        logging.error(f\"OpenCV error analyzing HSV: {e}\")\n        return None\n\n\ndef plot_color_histogram(roi, save_path, title_prefix=\"\"):\n    \"\"\"Calculates and saves the BGR color histogram for a ROI.\"\"\"\n    if roi is None or roi.size == 0:\n        logging.warning(f\"Cannot plot histogram for empty ROI. Save path: {save_path}\")\n        return\n    try:\n        plt.figure(figsize=(8, 5))\n        plt.suptitle(f'{title_prefix} Color Histogram'.strip(), fontsize=12)\n        colors = ('b', 'g', 'r')\n        max_pixels = 0\n        for i, color in enumerate(colors):\n            hist = cv2.calcHist([roi], [i], None, [256], [0, 256])\n            max_pixels = max(max_pixels, hist.max())\n            plt.plot(hist, color=color, label=f'{color.upper()} channel')\n\n        plt.title(f'Box Shape: {roi.shape[1]}x{roi.shape[0]}', fontsize=9, style='italic')\n        plt.xlabel('Pixel Value (0-255)')\n        plt.ylabel('Number of Pixels')\n        plt.xlim([0, 256])\n        plt.ylim([0, max_pixels * 1.1]) # Dynamic Y limit\n        plt.legend()\n        plt.grid(True, linestyle='--', alpha=0.6)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n        plt.savefig(save_path)\n        plt.close()\n        logging.debug(f\"Color histogram saved to {save_path}\")\n    except Exception as e:\n        logging.error(f\"Failed to plot/save histogram {save_path}: {e}\", exc_info=True)\n        plt.close() # Ensure plot is closed even on error\n\n\n# --- Visualization Helper ---\ndef draw_analysis_results(image, gt_data, pred_data, class_names):\n    \"\"\"\n    Draws GT and Prediction boxes with error type indication.\n\n    Args:\n        image: The input image (numpy array).\n        gt_data: List of GT box dicts {'box_xyxy':..., 'class_id':..., 'matched_pred_idx':...}\n        pred_data: List of Pred box dicts {'box_xyxy':..., 'class_id':..., 'score':..., 'matched_gt_idx':..., 'error_type':...}\n        class_names: List of class names.\n    \"\"\"\n    img_out = image.copy()\n    # Draw GT boxes\n    for i, gt in enumerate(gt_data):\n        x1, y1, x2, y2 = map(int, gt['box_xyxy'])\n        class_id = gt['class_id']\n        if not (0 <= class_id < len(class_names)): continue # Skip invalid\n        label = class_names[class_id]\n        color = (0, 255, 0) # Green (default = FN)\n        prefix = \"GT:\"\n        if gt.get('matched_pred_idx') is not None:\n              color = (0, 200, 0) # Darker green if matched\n              prefix = \"GT(M):\"\n\n        cv2.rectangle(img_out, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(img_out, f\"{prefix} {label}\", (x1, y1 - 10 if y1>10 else y1+15 ), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    # Draw Prediction boxes\n    for i, pred in enumerate(pred_data):\n        x1, y1, x2, y2 = map(int, pred['box_xyxy'])\n        class_id = pred['class_id']\n        score = pred['score']\n        error_type = pred.get('error_type', 'Unknown') # Default if somehow missing\n\n        if not (0 <= class_id < len(class_names)): continue # Skip invalid\n        pred_cls_name = class_names[class_id]\n        pred_label = f\"P: {pred_cls_name} ({score:.2f})\"\n\n        color = (128, 128, 128) # Default grey for unknown\n        if error_type == 'TP':\n            color = (255, 0, 0) # Blue\n        elif error_type == 'FP':\n            color = (0, 0, 255) # Red\n        elif error_type == 'Misclassification':\n            color = (0, 165, 255) # Orange\n            # Optionally add GT class if available\n            matched_gt_idx = pred.get('matched_gt_idx')\n            if matched_gt_idx is not None and 0 <= matched_gt_idx < len(gt_data):\n                gt_cls_id = gt_data[matched_gt_idx]['class_id']\n                if 0 <= gt_cls_id < len(class_names):\n                    pred_label += f\" (GT: {class_names[gt_cls_id]})\"\n\n        cv2.rectangle(img_out, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(img_out, pred_label, (x1, y2 + 15 if y2 < img_out.shape[0]-15 else y2-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    return img_out\n\nprint(\"Snippet 2 (Core Helpers) finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:21:52.239617Z","iopub.execute_input":"2025-04-24T09:21:52.239885Z","iopub.status.idle":"2025-04-24T09:21:52.261723Z","shell.execute_reply.started":"2025-04-24T09:21:52.239865Z","shell.execute_reply":"2025-04-24T09:21:52.261013Z"}},"outputs":[{"name":"stdout","text":"Snippet 2 (Core Helpers) finished.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- Snippet 3: Main Analysis Loop & Data Collection ---\n\ndef run_analysis(model, image_files, label_dir, class_names, dirs, conf_thresh, iou_thresh, target_fp_class_name=None):\n    \"\"\"Runs the main error analysis loop.\"\"\"\n    logging.info(f\"Starting main analysis loop for {len(image_files)} images...\")\n    num_classes = len(class_names)\n    device = next(model.parameters()).device # Get device model is on\n\n    # Data containers for TIDE\n    tide_gt_data = {} # {img_id: [{'bbox': [x1,y1,x2,y2], 'class': class_id}]}\n    tide_pred_data = [] # [{'bbox': [x1,y1,x2,y2], 'score': score, 'class': class_id, 'image_id': img_id}]\n\n    # Data containers for detailed analysis\n    all_results = [] # Store detailed info per image/box\n\n    # Optional: Target class ID for color analysis\n    target_fp_class_id = None\n    if target_fp_class_name:\n        try: target_fp_class_id = class_names.index(target_fp_class_name)\n        except ValueError: logging.warning(f\"Target FP class '{target_fp_class_name}' not found. Disabling color analysis.\")\n\n    for img_idx, img_path in enumerate(tqdm(image_files, desc=\"Analyzing Images\")):\n        image_id = img_path.stem # Use filename stem as image ID\n        logging.debug(f\"Processing Img {img_idx+1}/{len(image_files)}: {img_path.name}\")\n\n        try:\n            # --- Load Image & GT ---\n            img = cv2.imread(str(img_path))\n            if img is None:\n                logging.warning(f\"Could not read image {img_path}. Skipping.\")\n                continue\n            img_height, img_width = img.shape[:2]\n\n            label_path = label_dir / (img_path.stem + \".txt\")\n            gt_boxes_yolo = load_ground_truth(label_path, num_classes) # [cls, xc, yc, w, h]\n            gt_boxes_xyxy = yolo_to_xyxy(gt_boxes_yolo[:, 1:], img_width, img_height) # Get only coords\n\n            # Prepare GT for TIDE and detailed analysis\n            current_gt_list = []\n            tide_gt_list = []\n            for i, gt_yolo in enumerate(gt_boxes_yolo):\n                class_id = int(gt_yolo[0])\n                box_xyxy = gt_boxes_xyxy[i]\n                gt_entry = {'box_xyxy': box_xyxy.tolist(), 'class_id': class_id, 'matched_pred_idx': None}\n                current_gt_list.append(gt_entry)\n                tide_gt_list.append({'bbox': box_xyxy.tolist(), 'class': class_id})\n            tide_gt_data[image_id] = tide_gt_list\n\n\n            # --- Image Level Analysis ---\n            brightness = calculate_brightness(img)\n            contrast = calculate_contrast(img)\n            logging.debug(f\"Image {image_id}: Brightness={brightness:.2f}, Contrast={contrast:.2f}\")\n\n            # --- Prediction ---\n            results = model.predict(img_path, conf=conf_thresh, iou=iou_thresh, device=device, verbose=False)\n            preds = results[0] # Results for this image\n\n            pred_boxes_xyxy = preds.boxes.xyxy.cpu().numpy()\n            pred_scores = preds.boxes.conf.cpu().numpy()\n            pred_classes = preds.boxes.cls.cpu().numpy().astype(int)\n\n            # Prepare Predictions for TIDE and detailed analysis\n            current_pred_list = []\n            for i in range(len(pred_boxes_xyxy)):\n                 box_xyxy = pred_boxes_xyxy[i]\n                 score = pred_scores[i]\n                 class_id = pred_classes[i]\n                 pred_entry = {\n                     'box_xyxy': box_xyxy.tolist(),\n                     'score': float(score),\n                     'class_id': int(class_id),\n                     'matched_gt_idx': None,\n                     'error_type': None # Will be filled during matching\n                 }\n                 current_pred_list.append(pred_entry)\n                 tide_pred_data.append({\n                     'bbox': box_xyxy.tolist(),\n                     'score': float(score),\n                     'class': int(class_id),\n                     'image_id': image_id\n                 })\n\n            # --- Matching GT and Predictions ---\n            num_gt = len(current_gt_list)\n            num_pred = len(current_pred_list)\n            gt_matched_indices = set() # Indices of GT boxes that are matched\n            pred_matched_gt_indices = [None] * num_pred # Index of matched GT for each pred\n\n            if num_gt > 0 and num_pred > 0:\n                # Prepare boxes for IoU calculation\n                gt_xyxy_np = np.array([gt['box_xyxy'] for gt in current_gt_list])\n                pred_xyxy_np = np.array([p['box_xyxy'] for p in current_pred_list])\n\n                # Calculate IoU matrix (GT rows, Pred columns)\n                iou_matrix = box_iou(torch.tensor(gt_xyxy_np), torch.tensor(pred_xyxy_np)).numpy()\n\n                # Find potential matches above threshold\n                gt_idx_match, pred_idx_match = np.where(iou_matrix >= iou_thresh)\n                matches = np.array([gt_idx_match, pred_idx_match, iou_matrix[gt_idx_match, pred_idx_match]]).T\n\n                # Greedy matching based on highest IoU first\n                if len(matches) > 0:\n                    matches = matches[matches[:, 2].argsort()[::-1]] # Sort by IoU desc\n                    used_gt, used_pred = set(), set()\n                    for match in matches:\n                        gt_idx, pred_idx = int(match[0]), int(match[1])\n                        if gt_idx not in used_gt and pred_idx not in used_pred:\n                             # Record the match\n                             pred_matched_gt_indices[pred_idx] = gt_idx\n                             current_pred_list[pred_idx]['matched_gt_idx'] = gt_idx\n                             current_gt_list[gt_idx]['matched_pred_idx'] = pred_idx # Link back GT -> Pred\n                             gt_matched_indices.add(gt_idx)\n                             # Mark as used\n                             used_gt.add(gt_idx)\n                             used_pred.add(pred_idx)\n\n            # --- Categorize Errors ---\n            image_has_errors = False\n            # Process Predictions\n            for i, pred in enumerate(current_pred_list):\n                matched_gt_idx = pred['matched_gt_idx']\n                if matched_gt_idx is not None: # Matched prediction\n                    gt_class_id = current_gt_list[matched_gt_idx]['class_id']\n                    if pred['class_id'] == gt_class_id:\n                        pred['error_type'] = 'TP'\n                    else:\n                        pred['error_type'] = 'Misclassification'\n                        image_has_errors = True # Misclassification is an error\n                else: # Unmatched prediction\n                    pred['error_type'] = 'FP'\n                    image_has_errors = True # FP is an error\n\n            # Check for False Negatives (unmatched GT)\n            num_fn = 0\n            for i, gt in enumerate(current_gt_list):\n                 if gt['matched_pred_idx'] is None:\n                      num_fn += 1\n            if num_fn > 0: image_has_errors = True # FN is an error\n\n\n            # --- Store Detailed Results ---\n            all_results.append({\n                'image_id': image_id,\n                'image_path': str(img_path),\n                'width': img_width,\n                'height': img_height,\n                'brightness': float(brightness) if brightness != -1 else None,\n                'contrast': float(contrast) if contrast != -1 else None,\n                'ground_truth': current_gt_list,\n                'predictions': current_pred_list,\n                'has_errors': image_has_errors,\n                'fn_count': num_fn\n            })\n\n\n            # --- Save Error Image Visualization ---\n            if image_has_errors:\n                logging.debug(f\"Saving error image for {image_id}\")\n                img_to_draw = cv2.imread(str(img_path)) # Re-read to ensure clean image\n                drawn_img = draw_analysis_results(img_to_draw, current_gt_list, current_pred_list, class_names)\n\n                # Determine primary error type for directory saving\n                has_fn = num_fn > 0\n                has_fp = any(p['error_type'] == 'FP' for p in current_pred_list)\n                has_misc = any(p['error_type'] == 'Misclassification' for p in current_pred_list)\n\n                if has_fn: save_dir = dirs[\"errors_fn\"]\n                elif has_fp: save_dir = dirs[\"errors_fp\"]\n                elif has_misc: save_dir = dirs[\"errors_misc\"]\n                else: save_dir = OUTPUT_DIR # Fallback, should not happen if image_has_errors is True\n\n                error_flags = f\"{'FN' if has_fn else ''}{'FP' if has_fp else ''}{'MISC' if has_misc else ''}\"\n                save_path = save_dir / f\"err_{error_flags}_{image_id}.png\"\n                try:\n                    cv2.imwrite(str(save_path), drawn_img)\n                except Exception as write_e:\n                    logging.error(f\"Failed to write error image {save_path}: {write_e}\")\n\n            # --- Targeted FP Analysis (Color/HSV) ---\n            if target_fp_class_id is not None:\n                for i, pred in enumerate(current_pred_list):\n                    if pred['error_type'] == 'FP' and pred['class_id'] == target_fp_class_id:\n                        logging.debug(f\"Analyzing FP Box {i} (Class {target_fp_class_name}) in {image_id}\")\n                        fp_roi = get_box_roi(img, pred['box_xyxy'])\n                        if fp_roi is not None:\n                            # Plot Histogram\n                            hist_save_path = dirs[\"fp_color_analysis\"] / f\"hist_fp_{image_id}_pred{i}.png\"\n                            plot_color_histogram(fp_roi, hist_save_path, title_prefix=f\"FP {target_fp_class_name}\")\n\n                            # Analyze HSV (results could be added to all_results if needed)\n                            hsv_data = analyze_box_hsv(fp_roi)\n                            if hsv_data:\n                                logging.debug(f\"  FP Box HSV: H={hsv_data['H']:.1f}, S={hsv_data['S']:.1f}, V={hsv_data['V']:.1f}\")\n                                # Store this info if needed later: pred['hsv_avg'] = hsv_data\n\n        except Exception as e:\n            logging.error(f\"FATAL error processing image {img_path.name}: {e}\", exc_info=True)\n            # Optionally add to a list of failed images\n\n    logging.info(\"Finished main analysis loop.\")\n    return all_results, tide_gt_data, tide_pred_data\n\n\n# --- Placeholder for main execution logic ---\n# This snippet only defines the function. The next snippet will call it.\nprint(\"Snippet 3 (Main Loop Definition) finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:21:57.977809Z","iopub.execute_input":"2025-04-24T09:21:57.978390Z","iopub.status.idle":"2025-04-24T09:21:58.000183Z","shell.execute_reply.started":"2025-04-24T09:21:57.978364Z","shell.execute_reply":"2025-04-24T09:21:57.999384Z"}},"outputs":[{"name":"stdout","text":"Snippet 3 (Main Loop Definition) finished.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# --- Snippet 4: Execution, Summary, Slicing, Confidence, TIDE Analysis ---\n\ndef calculate_slice_metrics(results_slice):\n    \"\"\"Calculates metrics for a subset of results.\"\"\"\n    tp, fp, fn, misc = 0, 0, 0, 0\n    total_gt_in_slice = 0\n\n    for img_result in results_slice:\n        total_gt_in_slice += len(img_result['ground_truth'])\n        fn += img_result.get('fn_count', 0) # FN is per-image\n        for pred in img_result['predictions']:\n            if pred['error_type'] == 'TP': tp += 1\n            elif pred['error_type'] == 'FP': fp += 1\n            elif pred['error_type'] == 'Misclassification': misc += 1\n            # Note: FNs are counted based on unmatched GT, not directly from pred list\n\n    # Recalculate FN based on TP and Misc for consistency with overall GT count in slice\n    # Total GT = TP + FN_missed + FN_misclassified_location(Misc)\n    fn = total_gt_in_slice - tp - misc\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn + misc) if (tp + fn + misc) > 0 else 0 # Recall denominator is total GT objects\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    return {'TP': tp, 'FP': fp, 'FN': fn, 'Misc': misc, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Total GT': total_gt_in_slice, 'Num Images': len(results_slice)}\n\ndef plot_analysis_results(analysis_dict, title, xlabel, save_path):\n    \"\"\"Plots Precision, Recall, F1, and Support (Num Images) for analysis slices.\"\"\"\n    labels = list(analysis_dict.keys())\n    precision = [v['Precision'] for v in analysis_dict.values()]\n    recall = [v['Recall'] for v in analysis_dict.values()]\n    f1 = [v['F1'] for v in analysis_dict.values()]\n    support_gt = [v['Total GT'] for v in analysis_dict.values()]\n    support_img = [v['Num Images'] for v in analysis_dict.values()]\n\n    x = np.arange(len(labels))\n    width = 0.2\n\n    fig, ax1 = plt.subplots(figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    rects1 = ax1.bar(x - width, precision, width, label='Precision', color='skyblue')\n    rects2 = ax1.bar(x, recall, width, label='Recall', color='lightcoral')\n    rects3 = ax1.bar(x + width, f1, width, label='F1 Score', color='lightgreen')\n\n    ax1.set_ylabel('Scores (P, R, F1)', color='black')\n    ax1.set_xlabel(xlabel)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=45, ha=\"right\")\n    ax1.tick_params(axis='y', labelcolor='black')\n    ax1.legend(loc='upper left')\n    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n    ax1.set_ylim(0, 1.05)\n\n    # Add counts on a second y-axis\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Support (# GT Boxes / # Images)', color='dimgray')\n    # Plot support bars slightly offset or use lines/points\n    ax2.plot(x, support_gt, label='# GT Boxes', color='dimgray', linestyle='--', marker='o', markersize=5)\n    ax2.plot(x, support_img, label='# Images', color='darkgray', linestyle=':', marker='x', markersize=5)\n    ax2.tick_params(axis='y', labelcolor='dimgray')\n    ax2.legend(loc='upper right')\n    # ax2.set_ylim(0, max(max(support_gt), max(support_img)) * 1.2) # Adjust ylim for support\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n    plt.savefig(save_path)\n    plt.close(fig)\n    logging.info(f\"Analysis plot saved to {save_path}\")\n\n# --- Main Execution Block ---\nif __name__ == \"__main__\":\n    logging.info(\"Script execution started (main guard).\")\n    print(\"DEBUG: Script execution started in __main__.\")\n\n    model_exists = MODEL_PATH.exists()\n    yaml_exists = DATASET_YAML_PATH.exists()\n    print(f\"DEBUG: Checking Model Path: {MODEL_PATH} - Exists: {model_exists}\")\n    print(f\"DEBUG: Checking YAML Path: {DATASET_YAML_PATH} - Exists: {yaml_exists}\")\n\n    if not model_exists:\n        log_msg = f\"CRITICAL: Model file not found: {MODEL_PATH}. Exiting.\"\n        logging.error(log_msg); print(log_msg)\n    elif not yaml_exists:\n        log_msg = f\"CRITICAL: Dataset YAML not found: {DATASET_YAML_PATH}. Exiting.\"\n        logging.error(log_msg); print(log_msg)\n    else:\n        print(\"DEBUG: Pre-checks passed. Proceeding with analysis setup.\")\n        try:\n            # --- Setup ---\n            dirs = setup_output_dirs(OUTPUT_DIR)\n            model = YOLO(MODEL_PATH) # Load model here before analysis starts\n            image_files, label_dir, class_names = load_dataset_info(DATASET_YAML_PATH, SPLIT_TO_ANALYZE)\n\n            if not image_files:\n                 logging.error(\"No image files found. Aborting analysis.\")\n            else:\n                # --- Run Core Analysis ---\n                all_results, tide_gt_data, tide_pred_data = run_analysis(\n                    model, image_files, label_dir, class_names, dirs,\n                    CONF_THRESHOLD, IOU_THRESHOLD, TARGET_FP_ANALYSIS_CLASS_NAME\n                )\n\n                # --- Process Overall Results ---\n                logging.info(\"Calculating overall summary statistics...\")\n                overall_stats = calculate_slice_metrics(all_results) # Use helper for overall stats too\n                summary = {\n                    \"Total Images\": len(all_results),\n                    \"Total GT Boxes\": overall_stats['Total GT'],\n                    \"Total Predictions\": sum(len(r['predictions']) for r in all_results),\n                    \"TP\": overall_stats['TP'],\n                    \"FP\": overall_stats['FP'],\n                    \"FN (Missed + Misclassified)\": overall_stats['FN'] + overall_stats['Misc'], # Total GT not correctly identified\n                    \"FN (Missed Only)\": overall_stats['FN'],\n                    \"Misclassified\": overall_stats['Misc'],\n                    \"Precision\": overall_stats['Precision'],\n                    \"Recall\": overall_stats['Recall'],\n                    \"F1 Score\": overall_stats['F1'],\n                }\n                print(\"\\n\" + \"=\"*40)\n                print(\"--- Overall Analysis Summary ---\")\n                print(\"=\"*40)\n                for key, value in summary.items():\n                    if isinstance(value, float): print(f\"{key:<30}: {value:.4f}\")\n                    else: print(f\"{key:<30}: {value}\")\n                print(\"=\"*40)\n                # Save summary\n                summary_path = OUTPUT_DIR / \"overall_summary.json\"\n                with open(summary_path, 'w') as f: json.dump(summary, f, indent=4)\n                logging.info(f\"Overall summary saved to {summary_path}\")\n\n                # --- Perform Brightness Slicing Analysis ---\n                logging.info(\"Performing analysis sliced by image brightness...\")\n                brightness_slices = {\n                    f'Dark (<{BRIGHTNESS_DARK_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and r['brightness'] < BRIGHTNESS_DARK_THRESHOLD],\n                    f'Medium ({BRIGHTNESS_DARK_THRESHOLD}-{BRIGHTNESS_BRIGHT_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and BRIGHTNESS_DARK_THRESHOLD <= r['brightness'] <= BRIGHTNESS_BRIGHT_THRESHOLD],\n                    f'Bright (>{BRIGHTNESS_BRIGHT_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and r['brightness'] > BRIGHTNESS_BRIGHT_THRESHOLD],\n                    'Unknown/Failed': [r for r in all_results if r['brightness'] is None]\n                }\n                brightness_analysis = {}\n                for name, results_slice in brightness_slices.items():\n                    if results_slice: # Only analyze if slice has images\n                        brightness_analysis[name] = calculate_slice_metrics(results_slice)\n                        logging.info(f\"  Slice '{name}': {brightness_analysis[name]['Num Images']} images, Metrics: P={brightness_analysis[name]['Precision']:.3f}, R={brightness_analysis[name]['Recall']:.3f}, F1={brightness_analysis[name]['F1']:.3f}\")\n                    else:\n                         logging.info(f\"  Slice '{name}': 0 images.\")\n\n                # Plot brightness results\n                if brightness_analysis:\n                    plot_path = dirs[\"slice_analysis\"] / \"brightness_slice_performance.png\"\n                    plot_analysis_results(brightness_analysis, \"Performance across Image Brightness Slices\", \"Brightness Condition\", plot_path)\n\n                # --- Perform Confidence Bin Analysis ---\n                logging.info(\"Performing analysis sliced by prediction confidence...\")\n                confidence_analysis = {}\n                # Initialize bins\n                for i in range(len(CONFIDENCE_BINS) - 1):\n                     bin_label = f'{CONFIDENCE_BINS[i]:.1f}-{CONFIDENCE_BINS[i+1]:.1f}'\n                     confidence_analysis[bin_label] = {'TP': 0, 'FP': 0, 'Misc': 0, 'count': 0}\n\n                # Accumulate counts per bin\n                for img_result in all_results:\n                    for pred in img_result['predictions']:\n                         score = pred['score']\n                         error_type = pred['error_type']\n                         # Find correct bin\n                         for i in range(len(CONFIDENCE_BINS) - 1):\n                             if CONFIDENCE_BINS[i] <= score < CONFIDENCE_BINS[i+1]:\n                                 bin_label = f'{CONFIDENCE_BINS[i]:.1f}-{CONFIDENCE_BINS[i+1]:.1f}'\n                                 confidence_analysis[bin_label]['count'] += 1\n                                 if error_type == 'TP': confidence_analysis[bin_label]['TP'] += 1\n                                 elif error_type == 'FP': confidence_analysis[bin_label]['FP'] += 1\n                                 elif error_type == 'Misclassification': confidence_analysis[bin_label]['Misc'] += 1\n                                 break\n                         # Handle score == 1.0 edge case\n                         if score == 1.0 and len(CONFIDENCE_BINS) > 1:\n                              last_bin_label = f'{CONFIDENCE_BINS[-2]:.1f}-{CONFIDENCE_BINS[-1]:.1f}'\n                              confidence_analysis[last_bin_label]['count'] += 1\n                              if error_type == 'TP': confidence_analysis[last_bin_label]['TP'] += 1\n                              elif error_type == 'FP': confidence_analysis[last_bin_label]['FP'] += 1\n                              elif error_type == 'Misclassification': confidence_analysis[last_bin_label]['Misc'] += 1\n\n\n                # Calculate precision per bin\n                bin_labels_plot = []\n                bin_precision_plot = []\n                bin_support_plot = []\n                print(\"\\n--- Confidence Bin Analysis ---\")\n                print(f\"{'Confidence Bin':<15} {'Total Preds':<12} {'TP Rate':<10} {'FP Rate':<10} {'Misc Rate':<10} {'Precision':<10}\")\n                print(\"-\" * 70)\n                for label, data in confidence_analysis.items():\n                     count = data['count']\n                     if count > 0:\n                         tp_rate = data['TP'] / count\n                         fp_rate = data['FP'] / count\n                         misc_rate = data['Misc'] / count\n                         precision = data['TP'] / (data['TP'] + data['FP'] + data['Misc']) if (data['TP'] + data['FP'] + data['Misc']) > 0 else 0\n                         print(f\"{label:<15} {count:<12} {tp_rate:<10.3f} {fp_rate:<10.3f} {misc_rate:<10.3f} {precision:<10.3f}\")\n                         bin_labels_plot.append(label)\n                         bin_precision_plot.append(precision)\n                         bin_support_plot.append(count)\n                     else:\n                          print(f\"{label:<15} {count:<12} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10}\")\n\n                # Plot confidence results\n                if bin_labels_plot:\n                     plt.figure(figsize=(10, 6))\n                     plt.plot(bin_labels_plot, bin_precision_plot, marker='o', label='Precision per Bin')\n                     plt.xlabel(\"Confidence Bin\")\n                     plt.ylabel(\"Precision\")\n                     plt.title(\"Prediction Precision across Confidence Bins\")\n                     plt.ylim(0, 1.05)\n                     plt.grid(True, linestyle='--', alpha=0.7)\n                     # Add count as text\n                     for i, count in enumerate(bin_support_plot): plt.text(i, bin_precision_plot[i] + 0.02, f'n={count}', ha='center', va='bottom', fontsize=9)\n                     plt.tight_layout()\n                     plt.savefig(dirs[\"confidence_analysis\"] / \"confidence_bin_precision.png\")\n                     plt.close()\n                     logging.info(f\"Confidence analysis plot saved.\")\n\n\n                # --- Run TIDE Analysis ---\n                logging.info(\"Preparing and running TIDE analysis...\")\n                # Convert GT data for TIDE datasets API\n                tide_gt = datasets.DetectionResult('GroundTruth', image_ids=list(tide_gt_data.keys()))\n                for img_id, annotations in tide_gt_data.items():\n                    for ann in annotations:\n                        tide_gt.add_annotation(img_id, ann['class'], ann['bbox'])\n\n                # Convert Pred data for TIDE datasets API\n                tide_preds = datasets.DetectionResult('Predictions', image_ids=list(tide_gt_data.keys())) # Use GT image IDs\n                for pred_ann in tide_pred_data:\n                     # Ensure required keys exist\n                     if all(k in pred_ann for k in ['image_id', 'class', 'bbox', 'score']):\n                           tide_preds.add_prediction(pred_ann['image_id'], pred_ann['class'], pred_ann['bbox'], pred_ann['score'])\n                     else:\n                           logging.warning(f\"Skipping prediction due to missing keys: {pred_ann}\")\n\n\n                tide = TIDE()\n                # Note: Adjust TIDE pos_threshold if different from default 0.5 needed\n                # Note: background_threshold controls suppression, mode='bbox' for standard detection\n                try:\n                    tide.evaluate_range(tide_gt, tide_preds, mode=TIDE.BOX)\n                    summary_str = tide.get_summary() # Returns string\n                    print(\"\\n\" + \"=\"*40)\n                    print(\"--- TIDE Analysis Summary ---\")\n                    print(\"=\"*40)\n                    print(summary_str)\n                    print(\"=\"*40)\n\n                    # Save TIDE summary plot and detailed errors\n                    tide_plot_path = dirs[\"tide_results\"] / \"tide_summary_plot.png\"\n                    tide.plot(str(OUTPUT_DIR / dirs[\"tide_results\"])) # Pass folder path to save plots\n                    logging.info(f\"TIDE summary plot potentially saved in {dirs['tide_results']}\") # Plot function saves directly\n                    # Check if plot actually exists (tide.plot might fail silently sometimes)\n                    # if not tide_plot_path.exists():\n                    #      logging.warning(f\"TIDE plot file was not found at {tide_plot_path}. Plotting might have failed.\")\n\n                    # Save detailed TIDE errors to JSON\n                    tide_errors = tide.get_all_errors()\n                    tide_errors_path = dirs[\"tide_results\"] / \"tide_detailed_errors.json\"\n                    with open(tide_errors_path, 'w') as f: json.dump(tide_errors, f, indent=2)\n                    logging.info(f\"TIDE detailed errors saved to {tide_errors_path}\")\n\n                except Exception as tide_e:\n                    logging.error(f\"TIDE evaluation failed: {tide_e}\", exc_info=True)\n\n\n        except Exception as e:\n             logging.error(f\"An unexpected critical error occurred during analysis: {e}\", exc_info=True)\n             print(f\"CRITICAL ERROR during analysis: {e}\")\n\n    logging.info(\"Script execution finished (main guard).\")\n    print(\"DEBUG: Script execution finished in __main__.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:22:04.408095Z","iopub.execute_input":"2025-04-24T09:22:04.408659Z","iopub.status.idle":"2025-04-24T09:24:06.455893Z","shell.execute_reply.started":"2025-04-24T09:22:04.408634Z","shell.execute_reply":"2025-04-24T09:24:06.455337Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Script execution started in __main__.\nDEBUG: Checking Model Path: /kaggle/input/best-model-30/best.pt - Exists: True\nDEBUG: Checking YAML Path: /kaggle/input/cardetection/car/data.yaml - Exists: True\nDEBUG: Pre-checks passed. Proceeding with analysis setup.\n","output_type":"stream"},{"name":"stderr","text":"Analyzing Images: 100%|██████████| 801/801 [02:01<00:00,  6.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\n--- Overall Analysis Summary ---\n========================================\nTotal Images                  : 801\nTotal GT Boxes                : 944\nTotal Predictions             : 948\nTP                            : 828\nFP                            : 87\nFN (Missed + Misclassified)   : 116\nFN (Missed Only)              : 83\nMisclassified                 : 33\nPrecision                     : 0.9049\nRecall                        : 0.8771\nF1 Score                      : 0.8908\n========================================\n\n--- Confidence Bin Analysis ---\nConfidence Bin  Total Preds  TP Rate    FP Rate    Misc Rate  Precision \n----------------------------------------------------------------------\n0.0-0.3         33           0.333      0.485      0.182      0.333     \n0.3-0.5         73           0.233      0.493      0.274      0.233     \n0.5-0.7         65           0.600      0.369      0.031      0.600     \n0.7-0.9         118          0.873      0.085      0.042      0.873     \n0.9-1.0         659          0.998      0.002      0.000      0.998     \nCRITICAL ERROR during analysis: module 'tidecv.datasets' has no attribute 'DetectionResult'\nDEBUG: Script execution finished in __main__.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- Snippet 4: Execution, Summary, Slicing, Confidence, TIDE Analysis ---\n\ndef calculate_slice_metrics(results_slice):\n    \"\"\"Calculates metrics for a subset of results.\"\"\"\n    tp, fp, fn, misc = 0, 0, 0, 0\n    total_gt_in_slice = 0\n\n    for img_result in results_slice:\n        total_gt_in_slice += len(img_result['ground_truth'])\n        # FN count from the image result is the count of GT boxes not matched\n        fn += img_result.get('fn_count', 0)\n        for pred in img_result['predictions']:\n            if pred['error_type'] == 'TP': tp += 1\n            elif pred['error_type'] == 'FP': fp += 1\n            elif pred['error_type'] == 'Misclassification': misc += 1\n\n    # Validate counts: TP + FN (missed GT) + Misc (wrong class for GT) should equal total GT\n    # Note: The FN calculated here based on summing image FNs might differ slightly if\n    # the definition used in overall stats needs refinement, but should be close.\n    # Let's stick to the definition: Recall = TP / Total Actual Positives\n    total_actual_positives = tp + fn + misc # All GT boxes fall into one of these categories relative to preds\n    if total_actual_positives != total_gt_in_slice:\n         logging.warning(f\"Mismatch in GT counts! total_gt_in_slice={total_gt_in_slice}, tp+fn+misc={tp+fn+misc}\")\n         # Use total_gt_in_slice as the denominator for recall consistency\n         total_actual_positives = total_gt_in_slice\n\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    # Recall: How many of the actual objects did we find correctly?\n    recall = tp / total_actual_positives if total_actual_positives > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    return {'TP': tp, 'FP': fp, 'FN': fn, 'Misc': misc, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Total GT': total_gt_in_slice, 'Num Images': len(results_slice)}\n\ndef plot_analysis_results(analysis_dict, title, xlabel, save_path):\n    \"\"\"Plots Precision, Recall, F1, and Support (Num Images) for analysis slices.\"\"\"\n    labels = list(analysis_dict.keys())\n    precision = [v['Precision'] for v in analysis_dict.values()]\n    recall = [v['Recall'] for v in analysis_dict.values()]\n    f1 = [v['F1'] for v in analysis_dict.values()]\n    support_gt = [v['Total GT'] for v in analysis_dict.values()]\n    support_img = [v['Num Images'] for v in analysis_dict.values()]\n\n    x = np.arange(len(labels))\n    width = 0.2\n\n    fig, ax1 = plt.subplots(figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    rects1 = ax1.bar(x - width, precision, width, label='Precision', color='skyblue')\n    rects2 = ax1.bar(x, recall, width, label='Recall', color='lightcoral')\n    rects3 = ax1.bar(x + width, f1, width, label='F1 Score', color='lightgreen')\n\n    ax1.set_ylabel('Scores (P, R, F1)', color='black')\n    ax1.set_xlabel(xlabel)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=45, ha=\"right\")\n    ax1.tick_params(axis='y', labelcolor='black')\n    ax1.legend(loc='upper left')\n    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n    ax1.set_ylim(0, 1.05)\n\n    # Add counts on a second y-axis\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Support (# GT Boxes / # Images)', color='dimgray')\n    line1, = ax2.plot(x, support_gt, label='# GT Boxes', color='dimgray', linestyle='--', marker='o', markersize=5)\n    line2, = ax2.plot(x, support_img, label='# Images', color='darkgray', linestyle=':', marker='x', markersize=5)\n    ax2.tick_params(axis='y', labelcolor='dimgray')\n    # Combine legends\n    lines = [rects1[0], rects2[0], rects3[0], line1, line2]\n    labels_combined = [l.get_label() for l in lines]\n    ax1.legend(lines, labels_combined, loc='best') # Adjust legend location if needed\n    # ax2.legend(loc='upper right') # Remove separate legend for ax2\n\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n    plt.savefig(save_path)\n    plt.close(fig)\n    logging.info(f\"Analysis plot saved to {save_path}\")\n\n# --- Main Execution Block ---\nif __name__ == \"__main__\":\n    logging.info(\"Script execution started (main guard).\")\n    print(\"DEBUG: Script execution started in __main__.\")\n\n    model_exists = MODEL_PATH.exists()\n    yaml_exists = DATASET_YAML_PATH.exists()\n    print(f\"DEBUG: Checking Model Path: {MODEL_PATH} - Exists: {model_exists}\")\n    print(f\"DEBUG: Checking YAML Path: {DATASET_YAML_PATH} - Exists: {yaml_exists}\")\n\n    if not model_exists:\n        log_msg = f\"CRITICAL: Model file not found: {MODEL_PATH}. Exiting.\"\n        logging.error(log_msg); print(log_msg)\n    elif not yaml_exists:\n        log_msg = f\"CRITICAL: Dataset YAML not found: {DATASET_YAML_PATH}. Exiting.\"\n        logging.error(log_msg); print(log_msg)\n    else:\n        print(\"DEBUG: Pre-checks passed. Proceeding with analysis setup.\")\n        try:\n            # --- Setup ---\n            dirs = setup_output_dirs(OUTPUT_DIR)\n            print(\"DEBUG: Loading model...\") # Add debug print\n            model = YOLO(MODEL_PATH) # Load model here before analysis starts\n            print(\"DEBUG: Loading dataset info...\") # Add debug print\n            image_files, label_dir, class_names = load_dataset_info(DATASET_YAML_PATH, SPLIT_TO_ANALYZE)\n\n            if not image_files:\n                 logging.error(\"No image files found. Aborting analysis.\")\n                 print(\"ERROR: No image files found. Aborting analysis.\") # Add debug print\n            else:\n                # --- Run Core Analysis ---\n                print(f\"DEBUG: Calling run_analysis for {len(image_files)} images...\") # Add debug print\n                all_results, tide_gt_data, tide_pred_data = run_analysis(\n                    model, image_files, label_dir, class_names, dirs,\n                    CONF_THRESHOLD, IOU_THRESHOLD, TARGET_FP_ANALYSIS_CLASS_NAME\n                )\n                print(f\"DEBUG: run_analysis finished. Found {len(all_results)} image results.\") # Add debug print\n\n                # --- Process Overall Results ---\n                logging.info(\"Calculating overall summary statistics...\")\n                print(\"DEBUG: Calculating overall summary stats...\") # Add debug print\n                overall_stats = calculate_slice_metrics(all_results) # Use helper for overall stats too\n                summary = {\n                    \"Total Images\": len(all_results),\n                    \"Total GT Boxes\": overall_stats['Total GT'],\n                    \"Total Predictions\": sum(len(r['predictions']) for r in all_results),\n                    \"TP\": overall_stats['TP'],\n                    \"FP\": overall_stats['FP'],\n                    \"FN (Missed GT Only)\": overall_stats['FN'],\n                    \"Misclassified\": overall_stats['Misc'],\n                    \"Total GT Errors (FN + Misc)\": overall_stats['FN'] + overall_stats['Misc'],\n                    \"Precision\": overall_stats['Precision'],\n                    \"Recall\": overall_stats['Recall'],\n                    \"F1 Score\": overall_stats['F1'],\n                }\n                print(\"\\n\" + \"=\"*40)\n                print(\"--- Overall Analysis Summary ---\")\n                print(\"=\"*40)\n                for key, value in summary.items():\n                    if isinstance(value, float): print(f\"{key:<30}: {value:.4f}\")\n                    else: print(f\"{key:<30}: {value}\")\n                print(\"=\"*40)\n                # Save summary\n                summary_path = OUTPUT_DIR / \"overall_summary.json\"\n                with open(summary_path, 'w') as f: json.dump(summary, f, indent=4)\n                logging.info(f\"Overall summary saved to {summary_path}\")\n\n                # --- Perform Brightness Slicing Analysis ---\n                logging.info(\"Performing analysis sliced by image brightness...\")\n                print(\"DEBUG: Performing brightness slicing analysis...\") # Add debug print\n                brightness_slices = {\n                    f'Dark (<{BRIGHTNESS_DARK_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and r['brightness'] < BRIGHTNESS_DARK_THRESHOLD],\n                    f'Medium ({BRIGHTNESS_DARK_THRESHOLD}-{BRIGHTNESS_BRIGHT_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and BRIGHTNESS_DARK_THRESHOLD <= r['brightness'] <= BRIGHTNESS_BRIGHT_THRESHOLD],\n                    f'Bright (>{BRIGHTNESS_BRIGHT_THRESHOLD})': [r for r in all_results if r['brightness'] is not None and r['brightness'] > BRIGHTNESS_BRIGHT_THRESHOLD],\n                    'Unknown/Failed': [r for r in all_results if r['brightness'] is None]\n                }\n                brightness_analysis = {}\n                for name, results_slice in brightness_slices.items():\n                    if results_slice: # Only analyze if slice has images\n                        brightness_analysis[name] = calculate_slice_metrics(results_slice)\n                        logging.info(f\"  Slice '{name}': {brightness_analysis[name]['Num Images']} images, Metrics: P={brightness_analysis[name]['Precision']:.3f}, R={brightness_analysis[name]['Recall']:.3f}, F1={brightness_analysis[name]['F1']:.3f}\")\n                    else:\n                         logging.info(f\"  Slice '{name}': 0 images.\")\n\n                # Plot brightness results\n                if brightness_analysis:\n                    plot_path = dirs[\"slice_analysis\"] / \"brightness_slice_performance.png\"\n                    plot_analysis_results(brightness_analysis, \"Performance across Image Brightness Slices\", \"Brightness Condition\", plot_path)\n                    print(f\"DEBUG: Brightness slice plot saved to {plot_path}\") # Add debug print\n\n                # --- Perform Confidence Bin Analysis ---\n                logging.info(\"Performing analysis sliced by prediction confidence...\")\n                print(\"DEBUG: Performing confidence bin analysis...\") # Add debug print\n                confidence_analysis = {}\n                # Initialize bins\n                for i in range(len(CONFIDENCE_BINS) - 1):\n                     bin_label = f'{CONFIDENCE_BINS[i]:.1f}-{CONFIDENCE_BINS[i+1]:.1f}'\n                     confidence_analysis[bin_label] = {'TP': 0, 'FP': 0, 'Misc': 0, 'count': 0}\n\n                # Accumulate counts per bin\n                for img_result in all_results:\n                    for pred in img_result['predictions']:\n                         score = pred['score']\n                         error_type = pred['error_type']\n                         # Find correct bin\n                         assigned_bin = False\n                         for i in range(len(CONFIDENCE_BINS) - 1):\n                             if CONFIDENCE_BINS[i] <= score < CONFIDENCE_BINS[i+1]:\n                                 bin_label = f'{CONFIDENCE_BINS[i]:.1f}-{CONFIDENCE_BINS[i+1]:.1f}'\n                                 confidence_analysis[bin_label]['count'] += 1\n                                 if error_type == 'TP': confidence_analysis[bin_label]['TP'] += 1\n                                 elif error_type == 'FP': confidence_analysis[bin_label]['FP'] += 1\n                                 elif error_type == 'Misclassification': confidence_analysis[bin_label]['Misc'] += 1\n                                 assigned_bin = True\n                                 break\n                         # Handle score == 1.0 edge case or scores slightly outside range due to float issues\n                         if not assigned_bin and score >= CONFIDENCE_BINS[-1] - 1e-6: # Account for float precision near 1.0\n                              last_bin_label = f'{CONFIDENCE_BINS[-2]:.1f}-{CONFIDENCE_BINS[-1]:.1f}'\n                              confidence_analysis[last_bin_label]['count'] += 1\n                              if error_type == 'TP': confidence_analysis[last_bin_label]['TP'] += 1\n                              elif error_type == 'FP': confidence_analysis[last_bin_label]['FP'] += 1\n                              elif error_type == 'Misclassification': confidence_analysis[last_bin_label]['Misc'] += 1\n\n\n                # Calculate precision per bin\n                bin_labels_plot = []\n                bin_precision_plot = []\n                bin_support_plot = []\n                print(\"\\n--- Confidence Bin Analysis ---\")\n                print(f\"{'Confidence Bin':<15} {'Total Preds':<12} {'TP Rate':<10} {'FP Rate':<10} {'Misc Rate':<10} {'Precision':<10}\")\n                print(\"-\" * 70)\n                for label, data in confidence_analysis.items():\n                     count = data['count']\n                     if count > 0:\n                         tp_rate = data['TP'] / count\n                         fp_rate = data['FP'] / count\n                         misc_rate = data['Misc'] / count\n                         # Precision = TP / (TP + FP + Misc) for this bin\n                         precision = data['TP'] / count if count > 0 else 0 # Alt: TP Rate is kinda like precision if TP+FP+Misc = count\n                         precision_strict = data['TP'] / (data['TP'] + data['FP'] + data['Misc']) if (data['TP'] + data['FP'] + data['Misc']) > 0 else 0\n                         print(f\"{label:<15} {count:<12} {tp_rate:<10.3f} {fp_rate:<10.3f} {misc_rate:<10.3f} {precision_strict:<10.3f}\")\n                         bin_labels_plot.append(label)\n                         bin_precision_plot.append(precision_strict) # Plot strict precision\n                         bin_support_plot.append(count)\n                     else:\n                          print(f\"{label:<15} {count:<12} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10}\")\n\n                # Plot confidence results\n                if bin_labels_plot:\n                     conf_plot_path = dirs[\"confidence_analysis\"] / \"confidence_bin_precision.png\"\n                     plt.figure(figsize=(10, 6))\n                     plt.plot(bin_labels_plot, bin_precision_plot, marker='o', label='Precision per Bin')\n                     plt.xlabel(\"Confidence Bin\")\n                     plt.ylabel(\"Precision (TP / (TP+FP+Misc))\")\n                     plt.title(\"Prediction Precision across Confidence Bins\")\n                     plt.ylim(0, 1.05)\n                     plt.grid(True, linestyle='--', alpha=0.7)\n                     # Add count as text\n                     for i, count in enumerate(bin_support_plot): plt.text(i, bin_precision_plot[i] + 0.02, f'n={count}', ha='center', va='bottom', fontsize=9)\n                     plt.tight_layout()\n                     plt.savefig(conf_plot_path)\n                     plt.close()\n                     logging.info(f\"Confidence analysis plot saved to {conf_plot_path}\")\n                     print(f\"DEBUG: Confidence plot saved to {conf_plot_path}\") # Add debug print\n\n\n                # --- Run TIDE Analysis ---\n                logging.info(\"Preparing and running TIDE analysis...\")\n                print(\"DEBUG: Preparing TIDE data...\") # Add debug print\n                # Convert GT data for TIDE datasets API\n                tide_gt = datasets.DetectionResult('GroundTruth', image_ids=list(tide_gt_data.keys()))\n                for img_id, annotations in tide_gt_data.items():\n                    for ann in annotations:\n                        tide_gt.add_annotation(img_id, ann['class'], ann['bbox'])\n\n                # Convert Pred data for TIDE datasets API\n                tide_preds = datasets.DetectionResult('Predictions', image_ids=list(tide_gt_data.keys())) # Use GT image IDs\n                preds_added_tide = 0\n                for pred_ann in tide_pred_data:\n                     # Ensure required keys exist\n                     if all(k in pred_ann for k in ['image_id', 'class', 'bbox', 'score']):\n                           tide_preds.add_prediction(pred_ann['image_id'], pred_ann['class'], pred_ann['bbox'], pred_ann['score'])\n                           preds_added_tide += 1\n                     else:\n                           logging.warning(f\"Skipping prediction for TIDE due to missing keys: {pred_ann}\")\n                print(f\"DEBUG: Added {preds_added_tide} predictions to TIDE structure.\") # Add debug print\n\n                tide = TIDE()\n                # Note: Adjust TIDE pos_threshold if different from default 0.5 needed\n                # Note: background_threshold controls suppression, mode='bbox' for standard detection\n                try:\n                    print(\"DEBUG: Running TIDE evaluate_range...\") # Add debug print\n                    tide.evaluate_range(tide_gt, tide_preds, mode=TIDE.BOX)\n                    print(\"DEBUG: TIDE evaluate_range finished.\") # Add debug print\n                    summary_str = tide.get_summary() # Returns string\n                    print(\"\\n\" + \"=\"*40)\n                    print(\"--- TIDE Analysis Summary ---\")\n                    print(\"=\"*40)\n                    print(summary_str)\n                    print(\"=\"*40)\n\n                    # Save TIDE summary plot and detailed errors\n                    print(\"DEBUG: Plotting TIDE results...\") # Add debug print\n                    tide_plot_dir = str(dirs[\"tide_results\"]) # TIDE plot needs a folder path\n                    tide.plot(tide_plot_dir)\n                    logging.info(f\"TIDE summary plot saved in {tide_plot_dir}\")\n                    print(f\"DEBUG: TIDE plot saved in {tide_plot_dir}\") # Add debug print\n\n                    tide_errors = tide.get_all_errors()\n                    tide_errors_path = dirs[\"tide_results\"] / \"tide_detailed_errors.json\"\n                    with open(tide_errors_path, 'w') as f: json.dump(tide_errors, f, indent=2)\n                    logging.info(f\"TIDE detailed errors saved to {tide_errors_path}\")\n                    print(f\"DEBUG: TIDE errors saved to {tide_errors_path}\") # Add debug print\n\n                except Exception as tide_e:\n                    logging.error(f\"TIDE evaluation failed: {tide_e}\", exc_info=True)\n                    print(f\"ERROR: TIDE evaluation failed: {tide_e}\") # Add debug print\n\n\n        except Exception as e:\n             logging.error(f\"An unexpected critical error occurred during analysis setup or execution: {e}\", exc_info=True)\n             print(f\"CRITICAL ERROR during analysis setup/execution: {e}\") # Add debug print\n\n    logging.info(\"Script execution finished (main guard).\")\n    print(\"DEBUG: Script execution finished in __main__.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:27:43.432083Z","iopub.execute_input":"2025-04-24T09:27:43.432777Z","iopub.status.idle":"2025-04-24T09:29:27.049302Z","shell.execute_reply.started":"2025-04-24T09:27:43.432753Z","shell.execute_reply":"2025-04-24T09:29:27.048454Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Script execution started in __main__.\nDEBUG: Checking Model Path: /kaggle/input/best-model-30/best.pt - Exists: True\nDEBUG: Checking YAML Path: /kaggle/input/cardetection/car/data.yaml - Exists: True\nDEBUG: Pre-checks passed. Proceeding with analysis setup.\nDEBUG: Loading model...\nDEBUG: Loading dataset info...\nDEBUG: Calling run_analysis for 801 images...\n","output_type":"stream"},{"name":"stderr","text":"Analyzing Images: 100%|██████████| 801/801 [01:43<00:00,  7.77it/s]\n/tmp/ipykernel_31/279797592.py:71: UserWarning: The label '_nolegend_' of <matplotlib.patches.Rectangle object at 0x7b2d823e3850> starts with '_'. It is thus excluded from the legend.\n  ax1.legend(lines, labels_combined, loc='best') # Adjust legend location if needed\n/tmp/ipykernel_31/279797592.py:71: UserWarning: The label '_nolegend_' of <matplotlib.patches.Rectangle object at 0x7b2d82333610> starts with '_'. It is thus excluded from the legend.\n  ax1.legend(lines, labels_combined, loc='best') # Adjust legend location if needed\n/tmp/ipykernel_31/279797592.py:71: UserWarning: The label '_nolegend_' of <matplotlib.patches.Rectangle object at 0x7b2d823f7790> starts with '_'. It is thus excluded from the legend.\n  ax1.legend(lines, labels_combined, loc='best') # Adjust legend location if needed\n","output_type":"stream"},{"name":"stdout","text":"DEBUG: run_analysis finished. Found 801 image results.\nDEBUG: Calculating overall summary stats...\n\n========================================\n--- Overall Analysis Summary ---\n========================================\nTotal Images                  : 801\nTotal GT Boxes                : 944\nTotal Predictions             : 948\nTP                            : 828\nFP                            : 87\nFN (Missed GT Only)           : 83\nMisclassified                 : 33\nTotal GT Errors (FN + Misc)   : 116\nPrecision                     : 0.9049\nRecall                        : 0.8771\nF1 Score                      : 0.8908\n========================================\nDEBUG: Performing brightness slicing analysis...\nDEBUG: Brightness slice plot saved to /kaggle/working/robust_error_analysis/slice_analysis/brightness_slice_performance.png\nDEBUG: Performing confidence bin analysis...\n\n--- Confidence Bin Analysis ---\nConfidence Bin  Total Preds  TP Rate    FP Rate    Misc Rate  Precision \n----------------------------------------------------------------------\n0.0-0.3         33           0.333      0.485      0.182      0.333     \n0.3-0.5         73           0.233      0.493      0.274      0.233     \n0.5-0.7         65           0.600      0.369      0.031      0.600     \n0.7-0.9         118          0.873      0.085      0.042      0.873     \n0.9-1.0         659          0.998      0.002      0.000      0.998     \nDEBUG: Confidence plot saved to /kaggle/working/robust_error_analysis/confidence_analysis/confidence_bin_precision.png\nDEBUG: Preparing TIDE data...\nCRITICAL ERROR during analysis setup/execution: module 'tidecv.datasets' has no attribute 'DetectionResult'\nDEBUG: Script execution finished in __main__.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nimport zipfile\nfrom shutil import make_archive\n\n# Define the directory to be zipped\nworking_dir = '/kaggle/working'  # This is the default working directory in Kaggle Notebooks\n\n# Create a zip file from the working directory\nzip_filename = '/kaggle/working/working_directory.zip'\nmake_archive(zip_filename.replace('.zip', ''), 'zip', working_dir)\n\n# Now you can download the zip file via Kaggle's interface\nprint(f\"Zip file created: {zip_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:30:18.719736Z","iopub.execute_input":"2025-04-24T09:30:18.720416Z","iopub.status.idle":"2025-04-24T09:30:19.881057Z","shell.execute_reply.started":"2025-04-24T09:30:18.720393Z","shell.execute_reply":"2025-04-24T09:30:19.880392Z"}},"outputs":[{"name":"stdout","text":"Zip file created: /kaggle/working/working_directory.zip\n","output_type":"stream"}],"execution_count":23}]}